{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [
                {
                    "file_id": "1oInUJxh7vG7piq9t6Dj2HEVGDb5Mcj23",
                    "timestamp": 1732755945773
                }
            ]
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Notebook Explanation and Important Link\n",
                "This notebook follows the same data format as Aditya's, so you should be able to use the data by simply changing the directory or link, if necessary. <br>\n",
                "\n",
                "There are 3 main changes that have been made:\n",
                "1.   Instead of using B's videos (which was highly optimized for CNN), Wiame's videos are used. It maintains the original resolution but has standardized frames (113 frames). If the duration still seems too long, you can simply select 1 frame every 2 or 3 frames to reduce it.\n",
                "2.   Face Landmark Removal. Previously, in addition to Pose (33 landmarks) and Hands (21x2 landmarks), the Face model with 468 landmarks was included. However, due to the imbalance in the number of landmarks and the limited contribution of facial data to sign language recognition, it was removed.\n",
                "3.   When the pose or hand is not detected, instead of using a zero array as a padding, the previous detected coordinates are used to maintain continuity.\n",
                "\n",
                "Data Format<br>\n",
                "The output numpy array has the shape (113, 75, 3):<br>\n",
                "113 = Frame count <br>\n",
                "75 = Key points (0-32 pose, 33-53 left hand, 54-74 right hand). You can select specific hand indices if needed.<br>\n",
                "3 = Coordinates (x,y,z).\n",
                "\n",
                "Link:\n",
                "1.   [All data for and from this notebook, drive](https://drive.google.com/drive/folders/1rTRZxMkvAyf805AuPoVvrfw8KnB3Ttod?usp=share_link)\n",
                "2.   [Aditya original notebook, slack post](https://omdenaindones-9mu9399.slack.com/archives/C07MH4C0YLF/p1732443924936359)\n",
                "3.   [Wiame processed videos, slack post](https://omdenaindones-9mu9399.slack.com/archives/C07N05MQNCC/p1732105984337299)\n",
                "\n",
                "\n"
            ],
            "metadata": {
                "id": "mZnu4YU__0Xu"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Future Improvement\n",
                "\n",
                "1.   **Landmark-Level Augmentation.** Similar to video augmentation, but applied only to the coordinates. This includes mirroring, rotation, and adding noise.\n",
                "2.   **Model Result Comparison(Zero vs. Non-Zero).** A reference for future extraction, comparing results when using zero-filled coordinates versus using previously detected coordinates.\n",
                "3.   **Specific Hand and Pose Detection (vs. Holistic).** Focusing on specific hands and poses rather than holistic detection could allow for more flexible parameters, improving extraction performance and reducing landmark extraction duration.\n",
                "4.   **Confidence Parameter Adjustment.** Instead of using the default confidence threshold of 5, adjust it depending on the hand detection frequency. Lower it if hands are often not detected, or increase it for more precise results.\n",
                "5. **GPU Version?**\n",
                "\n",
                "\n"
            ],
            "metadata": {
                "id": "4TyJEm-eBaD-"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Install and Import Dependencies"
            ],
            "metadata": {
                "id": "APuxWetJyNYy"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install -q mediapipe"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "taep2Ct_yJYf",
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1733487093799,
                    "user_tz": -420,
                    "elapsed": 19819,
                    "user": {
                        "displayName": "Kenji Surya Utama",
                        "userId": "07896803662774016361"
                    }
                },
                "outputId": "5d9fa328-983f-4a3f-b9db-945d05ceda24"
            },
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25h"
                    ]
                }
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "n5vuIqvyxf6C"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import cv2\n",
                "import numpy as np\n",
                "import mediapipe as mp"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ],
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "hleMntX5ylbd",
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1733487127648,
                    "user_tz": -420,
                    "elapsed": 17459,
                    "user": {
                        "displayName": "Kenji Surya Utama",
                        "userId": "07896803662774016361"
                    }
                },
                "outputId": "33740e98-ed4e-4112-e72a-762c7534d2ad"
            },
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Mounted at /content/drive\n"
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Extract and Save Keypoints"
            ],
            "metadata": {
                "id": "5ttq5zr5yX91"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Non-Zero Extraction\n",
                "\n",
                "When the pose or hand is not detected, instead of using [0, 0, 0] to fill the coordinates, the previously detected coordinates are used. This way, the continuity of movement is preserved."
            ],
            "metadata": {
                "id": "QucGgt5ps2Dz"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Initialize Mediapipe Holistic\n",
                "mp_holistic = mp.solutions.holistic\n",
                "holistic = mp_holistic.Holistic(static_image_mode=False,\n",
                "                                min_detection_confidence=0.3,\n",
                "                                min_tracking_confidence=0.3)\n",
                "\n",
                "def extract_keypoints(video_path):\n",
                "\n",
                "    left_hand_keypoints = np.zeros((21, 3))\n",
                "    right_hand_keypoints = np.zeros((21, 3))\n",
                "    pose_keypoints = np.zeros((33, 3))\n",
                "\n",
                "    cap = cv2.VideoCapture(video_path)\n",
                "    keypoints_sequence = []\n",
                "\n",
                "    while cap.isOpened():\n",
                "        ret, frame = cap.read()\n",
                "        if not ret:\n",
                "            break\n",
                "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
                "        results = holistic.process(frame_rgb)\n",
                "\n",
                "        # If detected update the keypoints\n",
                "        # Extract pose landmarks\n",
                "        if results.pose_landmarks:\n",
                "            pose_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark])\n",
                "\n",
                "        # Extract left hand landmarks\n",
                "        if results.left_hand_landmarks:\n",
                "            left_hand_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark])\n",
                "        # Extract left hand landmarks\n",
                "        if results.right_hand_landmarks:\n",
                "            right_hand_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark])\n",
                "\n",
                "        # Concatenate all keypoints into a single vector\n",
                "        keypoints = np.concatenate([pose_keypoints, left_hand_keypoints, right_hand_keypoints])\n",
                "        keypoints_sequence.append(keypoints)\n",
                "\n",
                "    cap.release()\n",
                "\n",
                "    return keypoints_sequence # Shape: (num_frames, total_keypoints, 3)"
            ],
            "metadata": {
                "id": "S0Cj9GwDyWyn",
                "executionInfo": {
                    "status": "ok",
                    "timestamp": 1733488850362,
                    "user_tz": -420,
                    "elapsed": 276,
                    "user": {
                        "displayName": "Kenji Surya Utama",
                        "userId": "07896803662774016361"
                    }
                }
            },
            "execution_count": 7,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "DATA_DIR = '/content/drive/MyDrive/Omdena/sign_language_recognition/enhanced_videos_v2'\n",
                "SAVE_DIR = '/content/drive/MyDrive/Omdena/sign_language_recognition/landmark_non_zero'\n",
                "\n",
                "os.makedirs(SAVE_DIR, exist_ok=True)\n",
                "\n",
                "for word in os.listdir(DATA_DIR):\n",
                "    word_dir = os.path.join(DATA_DIR, word)\n",
                "    save_word_dir = os.path.join(SAVE_DIR, word)\n",
                "\n",
                "    os.makedirs(save_word_dir, exist_ok=True)\n",
                "    print(\"Processing\" , word, \"folder\")\n",
                "    for video_file in os.listdir(word_dir):\n",
                "        save_path = os.path.join(save_word_dir, video_file.replace('.mp4', '.npy'))\n",
                "\n",
                "        #Skip if the keypoints file already exists\n",
                "        if os.path.exists(save_path) and os.path.exists(save_path_zero):\n",
                "            continue\n",
                "\n",
                "        video_path = os.path.join(word_dir, video_file)\n",
                "        keypoints = extract_keypoints(video_path)\n",
                "        np.save(save_path, keypoints)  # Save as .npy"
            ],
            "metadata": {
                "id": "d0CIRajiyjYM",
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 362
                },
                "executionInfo": {
                    "status": "error",
                    "timestamp": 1733488890199,
                    "user_tz": -420,
                    "elapsed": 37317,
                    "user": {
                        "displayName": "Kenji Surya Utama",
                        "userId": "07896803662774016361"
                    }
                },
                "outputId": "92771245-803e-4d05-94d6-477f59a01d29"
            },
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Processing maaf folder\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-8-9f7c64298248>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mkeypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_keypoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Save as .npy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m<ipython-input-7-5b803b912212>\u001b[0m in \u001b[0;36mextract_keypoints\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mframe_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mholistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_rgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Extract pose landmarks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mediapipe/python/solutions/holistic.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \"\"\"\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlandmark\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mediapipe/python/solution_base.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    338\u001b[0m                                      data).at(self._simulated_timestamp))\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0;31m# Create a NamedTuple object where the field names are mapping to the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;31m# output stream names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Push to Dagshub\n",
                "\n"
            ],
            "metadata": {
                "id": "xvGFDKCtuSBN"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Install the DagsHub python client\n",
                "!pip install -q dagshub\n",
                "\n",
                "from dagshub.notebook import save_notebook\n",
                "\n",
                "save_notebook(repo=\"Omdena/JakartaIndonesia_SignLanguageTranslation\", path=\"preprocessing\", branch=\"kenji\")"
            ],
            "metadata": {
                "id": "UtPZKsiBVVkp"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Landmark Level Augmentation\n"
            ],
            "metadata": {
                "id": "c0asd4J-Dy9H"
            }
        }
    ]
}